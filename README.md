# Project JupitLunar: The GEO Content Engine

## 1. Vision & Purpose

This project is the definitive publishing front-end for the JupitLunar content ecosystem. Its sole purpose is to act as a **Generative Engine Optimization (GEO) signal amplifier**.

It is designed to receive pre-generated, expert-reviewed, and structured content from an external AI system. It then renders this content into hyper-optimized web pages and machine-readable feeds, designed for maximum discoverability, ingestion, and citation by AI search engines and Large Language Models (LLMs) like Google SGE, Perplexity, and ChatGPT.

**This is not a content management system (CMS); it is a content delivery and optimization engine.**

---

## 2. Core Architecture

The architecture is designed for simplicity, performance, and extreme optimization, based on a clear separation of concerns.

```mermaid
graph TD
    A[ğŸ¤– External AI Content System] -->|1. POST JSON Bundle| B(Next.js: /api/ingest);
    B -->|2. Call RPC| C[ğŸ˜ Supabase Database];
    C -->|upsert_article_bundle()| D[Tables: articles, qas, citations];

    subgraph "Next.js Frontend (Vercel)"
        E(User/Crawler/LLM) -->|4. Request Page| F(Dynamic Page: /[slug]);
        F -->|5. Fetch Data| G(Supabase View: v_article_bundles);
        G --> F;
        F -->|6. Render HTML + JSON-LD| E;

        H(LLM Aggregator) -->|Request Feed| I(API: /ai-feed.ndjson);
        I -->|Fetch Data| G;
        I -->|Serve NDJSON| H;
    end

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#9f9,stroke:#333,stroke-width:2px
```

-   **External AI System**: The "brain". Responsible for content generation, fact-checking, and adhering to the content model. It only needs to know one thing: how to post a JSON bundle to our `ingest` API.
-   **Next.js (`/api/ingest`)**: The "gatekeeper". A single, protected API endpoint that receives the content bundle. It performs zero content logic and immediately hands off the bundle to Supabase.
-   **Supabase (RPC Function)**: The "dispatcher". A powerful PostgreSQL function (`upsert_article_bundle`) handles the complex logic of atomically inserting or updating data across multiple tables (`articles`, `qas`, `citations`). This keeps the application logic clean and leverages the power of the database.
-   **Next.js (Pages)**: The "renderer". Its only job is to fetch fully-formed data from a Supabase View and render it using GEO-optimized components and inject the correct JSON-LD schemas. We use Static Site Generation (SSG) with `generateStaticParams` for maximum performance.
-   **Next.js (LLM Feeds)**: The "direct line". API routes like `/ai-feed.ndjson` and `/llm/answers.json` provide pre-digested, structured data specifically for machine consumption.

### How We Handle High-Volume Content (e.g., Daily News)

A core strength of this architecture is its ability to handle a massive volume of content without sacrificing performance or manual overhead.

-   **Automated Page Generation**: Every piece of content, including every daily news article, receives its own unique, permanent URL (e.g., `/news/2025/09/some-breaking-story`). This is handled automatically by Next.js's dynamic routing. You do **not** need to create a new page for each article.
-   **Performance with ISR**: We use **Incremental Static Regeneration (ISR)**. When a new article's URL is visited for the first time, Next.js generates the page and saves it as a static file. All subsequent visitors get this ultra-fast static file instantly. The page can be periodically and automatically re-validated in the background to ensure content freshness.
-   **Maximum Freshness Signal**: This approach of constantly creating new, unique pages is the strongest possible "freshness" signal we can send to AI crawlers, encouraging them to visit and index our site frequently.

---

## 3. Getting Started

### Prerequisites

-   Node.js (v18 or later)
-   npm / pnpm / yarn
-   A Supabase project
-   (Optional) A Firebase project for image storage

### Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd <your-repo-name>
    ```

2.  **Install dependencies:**
    ```bash
    npm install
    ```

3.  **Set up environment variables:**
    Create a file named `.env.local` in the project root and copy the contents of `.env.local.example`.

    ```sh
    # .env.local
    
    # Supabase
    NEXT_PUBLIC_SUPABASE_URL=YOUR_SUPABASE_PROJECT_URL
    NEXT_PUBLIC_SUPABASE_ANON_KEY=YOUR_SUPABASE_ANON_KEY
    SUPABASE_SERVICE_ROLE_KEY=YOUR_SUPABASE_SERVICE_ROLE_KEY # For admin tasks

    # Ingest API
    INGEST_API_SECRET=GENERATE_A_STRONG_SECRET_KEY # Secret key for the /api/ingest endpoint

    # (Optional) Firebase - if you use it for images
    # NEXT_PUBLIC_FIREBASE_...=...
    
    # (Optional) Vercel Deploy Hook for re-validation
    VERCEL_DEPLOY_HOOK_URL=YOUR_VERCEL_DEPLOY_HOOK_URL
    ```

4.  **Set up Supabase Database:**
    -   Navigate to your Supabase project's SQL Editor.
    -   Open the `supabase_schema.sql` file from this repository.
    -   Copy its contents and run it in the SQL Editor. This will create all the necessary tables, views, and the `upsert_article_bundle` function.

5.  **Run the development server:**
    ```bash
    npm run dev
    ```
    Open [http://localhost:3000](http://localhost:3000) to view the site.

---

## 4. API Usage: The Ingest Endpoint

This is the single point of contact for your external AI content system.

-   **Endpoint**: `POST /api/ingest`
-   **Authentication**: `Authorization: Bearer <YOUR_INGEST_API_SECRET>`
-   **Body**: A JSON object representing the `articleBundle`.

### `articleBundle` JSON Schema

```json
{
  "slug": "ferber-basics", // Unique identifier for the article (string)
  "type": "howto", // 'explainer', 'howto', 'research', 'faq', 'recipe', 'news' (string)
  "hub": "sleep", // The content hub this belongs to (string)
  "lang": "zh", // 'en' or 'zh' (string)
  "title": "Ferber ç¡çœ è®­ç»ƒå…¥é—¨", // (string)
  "one_liner": "é€æ­¥å»¶é•¿å®‰æŠšé—´éš”ï¼Œå¸®åŠ©å®å®å­¦ä¼šè‡ªä¸»å…¥ç¡ã€‚", // 50-80 char TL;DR (string)
  "key_facts": [ // 3-6 key takeaways (array of strings)
    "é€‚åˆå·²å…·å¤‡è‡ªæˆ‘å®‰æŠšè¿¹è±¡çš„å®å®ï¼›ä»çŸ­é—´éš”å¼€å§‹",
    "æ¯æ¬¡å®‰æŠšâ‰¤1åˆ†é’Ÿï¼Œä¸æŠ±èµ·ã€ä¸å¼€ç¯",
    "è¿ç»­æ‰§è¡Œ 3â€“7 å¤©è¯„ä¼°æ•ˆæœ"
  ],
  "age_range": "6â€“12ä¸ªæœˆ", // (string, optional)
  "region": "CA/US", // (string)
  "last_reviewed": "2025-08-31", // YYYY-MM-DD format (string)
  "reviewed_by": "å„¿ç§‘é¡¾é—® Jane Doe, MD", // E-E-A-T signal (string)
  "entities": ["sleep training", "Ferber method"], // SEO/GEO entities (array of strings)
  "license": "CC BY-NC", // (string)
  "published_at": "2025-09-01T10:00:00Z", // ISO 8601 format (string)
  "body_md": "## åŸç†\nFerberæ–¹æ³•çš„æ ¸å¿ƒæ˜¯...", // Main content in Markdown (string, optional)
  "steps": [ // For 'howto' and 'recipe' types (array of objects, optional)
    { "title": "è§‚å¯Ÿå›°å€¦ä¿¡å·", "text": "..." },
    { "title": "è®¾å®šåˆå§‹é—´éš”", "text": "..." }
  ],
  "faq": [ // For structured FAQ data (array of objects, optional)
    { "q": "ç¬¬ä¸€å¤©çš„å®‰æŠšé—´éš”æ€ä¹ˆè®¾ï¼Ÿ", "a": "...", "url": "#steps" }
  ],
  "citations": [ // For claims and references (array of objects, optional)
    { "claim": "...", "title": "AAP HealthyChildren", "url": "https://..." }
  ]
}
```

---

## 5. Deployment

This project is optimized for deployment on [Vercel](https://vercel.com/).

1.  Connect your Git repository to Vercel.
2.  Set up the same environment variables from your `.env.local` file in the Vercel project settings.
3.  Vercel will automatically detect it's a Next.js project and deploy it.
4.  For content updates, your AI system can call the `ingest` API and optionally trigger a Vercel deploy hook to rebuild the site with the new static content.
